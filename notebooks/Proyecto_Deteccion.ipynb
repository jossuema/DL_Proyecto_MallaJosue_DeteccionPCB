
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Detecci√≥n de Componentes Electr√≥nicos en PCB: YOLOv8 vs RT-DETR\n",
    "## Proyecto de Deep Learning - Detecci√≥n de Objetos\n",
    "\n",
    "**Autor:** [Tu Nombre]\n",
    "**Dataset:** Printed Circuit Board Defects - Roboflow Universe\n",
    "**Modelos:** YOLOv8s, RT-DETR\n",
    "**Fecha:** [Fecha de ejecuci√≥n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup e Instalaci√≥n de Librer√≠as\n",
    "\n",
    "Instalamos todas las librer√≠as necesarias para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "# Instalaci√≥n de librer√≠as principales\n",
    "!pip install ultralytics roboflow transformers torch torchvision\n",
    "!pip install opencv-python matplotlib seaborn pandas numpy\n",
    "!pip install albumentations pillow requests tqdm\n",
    "\n",
    "# Verificar GPU\n",
    "import torch\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Ultralytics YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Transformers para RT-DETR\n",
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_section"
   },
   "source": [
    "## 2. Carga y Exploraci√≥n del Dataset PCB\n",
    "\n",
    "Descargamos y exploramos el dataset de defectos en placas de circuito impreso desde Roboflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_download"
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n de Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")  # Reemplazar con tu API key\n",
    "\n",
    "# Descargar dataset PCB\n",
    "project = rf.workspace(\"roboflow-100\").project(\"printed-circuit-board\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "\n",
    "# Mostrar estructura del dataset\n",
    "dataset_path = dataset.location\n",
    "print(f\"Dataset descargado en: {dataset_path}\")\n",
    "\n",
    "# Explorar estructura\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    level = root.replace(dataset_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Mostrar solo primeros 5 archivos\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... y {len(files)-5} archivos m√°s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_analysis"
   },
   "outputs": [],
   "source": [
    "# An√°lisis del dataset\n",
    "data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
    "\n",
    "# Leer configuraci√≥n del dataset\n",
    "import yaml\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìä INFORMACI√ìN DEL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"N√∫mero de clases: {data_config['nc']}\")\n",
    "print(f\"Clases: {data_config['names']}\")\n",
    "print(f\"Ruta train: {data_config['train']}\")\n",
    "print(f\"Ruta val: {data_config['val']}\")\n",
    "print(f\"Ruta test: {data_config['test']}\")\n",
    "\n",
    "# Contar im√°genes en cada split\n",
    "train_images = len(os.listdir(os.path.join(dataset_path, 'train', 'images')))\n",
    "val_images = len(os.listdir(os.path.join(dataset_path, 'valid', 'images')))\n",
    "test_images = len(os.listdir(os.path.join(dataset_path, 'test', 'images')))\n",
    "\n",
    "print(f\"\nüìà DISTRIBUCI√ìN DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Im√°genes de entrenamiento: {train_images}\")\n",
    "print(f\"Im√°genes de validaci√≥n: {val_images}\")\n",
    "print(f\"Im√°genes de prueba: {test_images}\")\n",
    "print(f\"Total: {train_images + val_images + test_images}\")\n",
    "\n",
    "# Guardar informaci√≥n para uso posterior\n",
    "dataset_info = {\n",
    "    'path': dataset_path,\n",
    "    'classes': data_config['names'],\n",
    "    'nc': data_config['nc'],\n",
    "    'train_size': train_images,\n",
    "    'val_size': val_images,\n",
    "    'test_size': test_images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_samples"
   },
   "outputs": [],
   "source": [
    "# Visualizar muestras del dataset\n",
    "def plot_sample_images_with_labels(dataset_path, split='train', num_samples=6):\n",
    "    \"\"\"Visualiza im√°genes de muestra con sus etiquetas\"\"\"\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, split, 'images')\n",
    "    labels_dir = os.path.join(dataset_path, split, 'labels')\n",
    "    \n",
    "    image_files = os.listdir(images_dir)[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    class_names = data_config['names']\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        # Cargar imagen\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Cargar etiquetas\n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Dibujar bounding boxes\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                \n",
    "                # Convertir a coordenadas de p√≠xeles\n",
    "                x1 = int((x_center - width/2) * w)\n",
    "                y1 = int((y_center - height/2) * h)\n",
    "                x2 = int((x_center + width/2) * w)\n",
    "                y2 = int((y_center + height/2) * h)\n",
    "                \n",
    "                # Dibujar rect√°ngulo\n",
    "                color = colors[class_id][:3]  # RGB\n",
    "                color = tuple(int(c * 255) for c in color)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # A√±adir etiqueta\n",
    "                label = class_names[class_id]\n",
    "                cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.5, color, 1)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"Imagen {idx+1}: {img_file}\")\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Muestras del conjunto de {split.upper()}\", y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar muestras\n",
    "plot_sample_images_with_labels(dataset_path, 'train', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 3. Preprocesamiento y Data Augmentation\n",
    "\n",
    "Configuramos las transformaciones y augmentaciones para mejorar la robustez del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "augmentation_setup"
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n de augmentaciones usando Albumentations\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.4),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.2, \n",
    "        contrast_limit=0.2, \n",
    "        p=0.5\n",
    "    ),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n de augmentaciones completada\")\n",
    "print(\"\nTransformaciones de entrenamiento:\")\n",
    "for transform in train_transforms.transforms:\n",
    "    print(f\"  - {transform.__class__.__name__}\")\n",
    "\n",
    "print(\"\nTransformaciones de validaci√≥n:\")\n",
    "for transform in val_transforms.transforms:\n",
    "    print(f\"  - {transform.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_augmentations"
   },
   "outputs": [],
   "source": [
    "# Demostraci√≥n de augmentaciones\n",
    "def show_augmentation_effects(dataset_path, num_examples=3):\n",
    "    \"\"\"Muestra el efecto de las augmentaciones en im√°genes de muestra\"\"\"\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'train', 'images')\n",
    "    labels_dir = os.path.join(dataset_path, 'train', 'labels')\n",
    "    \n",
    "    image_files = os.listdir(images_dir)[:num_examples]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4*num_examples))\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        # Cargar imagen original\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        original_img = cv2.imread(img_path)\n",
    "        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Cargar etiquetas (simplificado para demostraci√≥n)\n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        \n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    class_labels.append(int(parts[0]))\n",
    "                    bboxes.append([float(x) for x in parts[1:5]])\n",
    "        \n",
    "        # Mostrar imagen original\n",
    "        axes[i, 0].imshow(original_img)\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Aplicar diferentes augmentaciones\n",
    "        aug_names = ['Flip + Rotate', 'Brightness/Contrast', 'Noise + Blur']\n",
    "        \n",
    "        for j in range(3):\n",
    "            if bboxes:\n",
    "                augmented = train_transforms(image=original_img.copy(), \n",
    "                                           bboxes=bboxes, \n",
    "                                           class_labels=class_labels)\n",
    "                aug_img = augmented['image']\n",
    "            else:\n",
    "                aug_img = train_transforms(image=original_img.copy(), \n",
    "                                         bboxes=[], \n",
    "                                         class_labels=[])['image']\n",
    "            \n",
    "            # Desnormalizar para visualizaci√≥n\n",
    "            aug_img = aug_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            aug_img = np.clip(aug_img, 0, 1)\n",
    "            \n",
    "            axes[i, j+1].imshow(aug_img)\n",
    "            axes[i, j+1].set_title(aug_names[j])\n",
    "            axes[i, j+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Efectos de Data Augmentation', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar efectos de augmentaci√≥n\n",
    "show_augmentation_effects(dataset_path, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 4. Entrenamiento de Modelos\n",
    "\n",
    "Entrenamos tres variantes: YOLOv8s y RT-DETR con configuraciones est√°ndar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yolo_training"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento YOLOv8s\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO YOLOv8s\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cargar modelo base\n",
    "yolo_model = YOLO('yolov8s.pt')  # Carga el modelo preentrenado\n",
    "\n",
    "# Configuraci√≥n de entrenamiento\n",
    "yolo_results = yolo_model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='yolov8s_pcb',\n",
    "    save_period=10,\n",
    "    patience=15,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    weight_decay=0.0005,\n",
    "    augment=True,\n",
    "    mosaic=0.8,\n",
    "    mixup=0.1,\n",
    "    copy_paste=0.1,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=8,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento YOLOv8s completado\")\n",
    "\n",
    "# Guardar ruta del modelo entrenado\n",
    "yolo_model_path = yolo_model.trainer.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtdetr_setup"
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n RT-DETR\n",
    "print(\"üöÄ CONFIGURANDO RT-DETR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Nota: RT-DETR tambi√©n puede entrenarse usando Ultralytics\n",
    "# que simplifica enormemente el proceso\n",
    "\n",
    "# Cargar RT-DETR usando ultralytics\n",
    "rtdetr_model = YOLO('rtdetr-l.pt')  # Carga RT-DETR preentrenado\n",
    "\n",
    "print(\"‚úÖ Modelo RT-DETR cargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtdetr_training"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento RT-DETR\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO RT-DETR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuraci√≥n de entrenamiento para RT-DETR\n",
    "rtdetr_results = rtdetr_model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=8,  # Batch menor por ser un modelo m√°s pesado\n",
    "    name='rtdetr_pcb',\n",
    "    save_period=10,\n",
    "    patience=15,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.0001,  # Learning rate m√°s bajo para transformers\n",
    "    weight_decay=0.0001,\n",
    "    augment=True,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=4,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento RT-DETR completado\")\n",
    "\n",
    "# Guardar ruta del modelo entrenado\n",
    "rtdetr_model_path = rtdetr_model.trainer.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 5. Evaluaci√≥n y M√©tricas\n",
    "\n",
    "Evaluamos todos los modelos con m√∫ltiples m√©tricas: mAP@0.5, mAP@[0.5:0.95], Precision por clase, Recall por clase, y velocidad de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluaci√≥n completa de modelos\n",
    "def evaluate_model(model_path, model_name, data_yaml):\n",
    "    \"\"\"Eval√∫a un modelo entrenado y retorna m√©tricas detalladas\"\"\"\n",
    "    \n",
    "    print(f\"\nüìä EVALUANDO {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Cargar modelo entrenado\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Validaci√≥n en conjunto de test\n",
    "    results = model.val(\n",
    "        data=data_yaml,\n",
    "        split='test',\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        verbose=True,\n",
    "        save_json=True,\n",
    "        name=f'{model_name}_eval'\n",
    "    )\n",
    "    \n",
    "    # Extraer m√©tricas\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'mAP_50': results.box.map50,\n",
    "        'mAP_50_95': results.box.map,\n",
    "        'precision': results.box.mp,\n",
    "        'recall': results.box.mr,\n",
    "        'f1_score': 2 * (results.box.mp * results.box.mr) / (results.box.mp + results.box.mr) if (results.box.mp + results.box.mr) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # M√©tricas por clase\n",
    "    class_metrics = {\n",
    "        'precision_per_class': results.box.p,\n",
    "        'recall_per_class': results.box.r,\n",
    "        'ap50_per_class': results.box.ap50,\n",
    "        'ap_per_class': results.box.ap\n",
    "    }\n",
    "    \n",
    "    return metrics, class_metrics, results\n",
    "\n",
    "# Evaluaci√≥n de velocidad de inferencia\n",
    "def benchmark_inference_speed(model_path, dataset_path, num_images=100):\n",
    "    \"\"\"Mide la velocidad de inferencia del modelo\"\"\"\n",
    "    \n",
    "    model = YOLO(model_path)\n",
    "    test_images_dir = os.path.join(dataset_path, 'test', 'images')\n",
    "    test_images = os.listdir(test_images_dir)[:num_images]\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Midiendo velocidad de inferencia en {len(test_images)} im√°genes...\")\n",
    "    \n",
    "    for img_file in test_images:\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        \n",
    "        # Medir tiempo de inferencia\n",
    "        start_time = time.time()\n",
    "        results = model(img_path, verbose=False)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': avg_time,\n",
    "        'fps': fps,\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'std_time': np.std(times)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funciones de evaluaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluations"
   },
   "outputs": [],
   "source": [
    "# Ejecutar evaluaciones\n",
    "import time\n",
    "\n",
    "# Diccionario para almacenar todas las m√©tricas\n",
    "all_metrics = {}\n",
    "all_class_metrics = {}\n",
    "inference_speeds = {}\n",
    "\n",
    "# Lista de modelos para evaluar\n",
    "models_to_evaluate = [\n",
    "    (yolo_model_path, 'YOLOv8s'),\n",
    "    (rtdetr_model_path, 'RT-DETR-L')\n",
    "]\n",
    "\n",
    "# Evaluar cada modelo\n",
    "for model_path, model_name in models_to_evaluate:\n",
    "    # M√©tricas de precisi√≥n\n",
    "    metrics, class_metrics, results = evaluate_model(model_path, model_name, data_yaml_path)\n",
    "    all_metrics[model_name] = metrics\n",
    "    all_class_metrics[model_name] = class_metrics\n",
    "    \n",
    "    # Velocidad de inferencia\n",
    "    speed_metrics = benchmark_inference_speed(model_path, dataset_path)\n",
    "    inference_speeds[model_name] = speed_metrics\n",
    "    \n",
    "    print(f\"\n‚úÖ {model_name} evaluado completamente\")\n",
    "\n",
    "print(\"\nüéâ TODAS LAS EVALUACIONES COMPLETADAS\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "# Mostrar resultados tabulados\n",
    "def display_comprehensive_results(all_metrics, inference_speeds, class_names):\n",
    "    \"\"\"Muestra resultados completos de evaluaci√≥n\"\"\"\n",
    "    \n",
    "    print(\"\nüìä RESUMEN COMPLETO DE RESULTADOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas generales\n",
    "    results_data = []\n",
    "    \n",
    "    for model_name in all_metrics.keys():\n",
    "        row = {\n",
    "            'Modelo': model_name,\n",
    "            'mAP@0.5': f\"{all_metrics[model_name]['mAP_50']:.3f}\",\n",
    "            'mAP@0.5:0.95': f\"{all_metrics[model_name]['mAP_50_95']:.3f}\",\n",
    "            'Precision': f\"{all_metrics[model_name]['precision']:.3f}\",\n",
    "            'Recall': f\"{all_metrics[model_name]['recall']:.3f}\",\n",
    "            'F1-Score': f\"{all_metrics[model_name]['f1_score']:.3f}\",\n",
    "            'FPS': f\"{inference_speeds[model_name]['fps']:.1f}\",\n",
    "            'Tiempo (ms)': f\"{inference_speeds[model_name]['avg_inference_time']*1000:.1f}\"\n",
    "        }\n",
    "        results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # M√©tricas por clase\n",
    "    print(\"\n\nüìà M√âTRICAS POR CLASE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model_name in all_metrics.keys():\n",
    "        print(f\"\n{model_name}:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        class_data = []\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            if i < len(all_class_metrics[model_name]['precision_per_class']):\n",
    "                row = {\n",
    "                    'Clase': class_name,\n",
    "                    'Precision': f\"{all_class_metrics[model_name]['precision_per_class'][i]:.3f}\",\n",
    "                    'Recall': f\"{all_class_metrics[model_name]['recall_per_class'][i]:.3f}\",\n",
    "                    'AP@0.5': f\"{all_class_metrics[model_name]['ap50_per_class'][i]:.3f}\",\n",
    "                    'AP@0.5:0.95': f\"{all_class_metrics[model_name]['ap_per_class'][i]:.3f}\"\n",
    "                }\n",
    "                class_data.append(row)\n",
    "        \n",
    "        class_df = pd.DataFrame(class_data)\n",
    "        print(class_df.to_string(index=False))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Mostrar resultados\n",
    "results_table = display_comprehensive_results(all_metrics, inference_speeds, data_config['names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization_section"
   },
   "source": [
    "## 6. Visualizaci√≥n de Resultados\n",
    "\n",
    "Creamos gr√°ficas comparativas de m√©tricas por modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_comparisons"
   },
   "outputs": [],
   "source": [
    "# Gr√°ficas comparativas de m√©tricas\n",
    "def plot_model_comparison(all_metrics, inference_speeds):\n",
    "    \"\"\"Crea gr√°ficas comparativas entre modelos\"\"\"\n",
    "    \n",
    "    models = list(all_metrics.keys())\n",
    "    \n",
    "    # Preparar datos\n",
    "    metrics_data = {\n",
    "        'mAP@0.5': [all_metrics[model]['mAP_50'] for model in models],\n",
    "        'mAP@0.5:0.95': [all_metrics[model]['mAP_50_95'] for model in models],\n",
    "        'Precision': [all_metrics[model]['precision'] for model in models],\n",
    "        'Recall': [all_metrics[model]['recall'] for model in models],\n",
    "        'F1-Score': [all_metrics[model]['f1_score'] for model in models]\n",
    "    }\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Comparaci√≥n de Rendimiento entre Modelos', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Colores para los modelos\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Gr√°fica de barras para m√©tricas principales\n",
    "    metric_names = list(metrics_data.keys())\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, metric in enumerate(metric_names):\n",
    "        ax = axes[i//3, i%3]\n",
    "        bars = ax.bar(x, metrics_data[metric], width*3, color=colors[:len(models)], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Modelos')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} por Modelo')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir valores en las barras\n",
    "        for bar, value in zip(bars, metrics_data[metric]):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Gr√°fica de velocidad de inferencia\n",
    "    ax = axes[1, 2]\n",
    "    fps_values = [inference_speeds[model]['fps'] for model in models]\n",
    "    bars = ax.bar(models, fps_values, color=colors[:len(models)], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Modelos')\n",
    "    ax.set_ylabel('FPS')\n",
    "    ax.set_title('Velocidad de Inferencia (FPS)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars, fps_values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "               f'{value:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gr√°fico radar para comparaci√≥n multidimensional\n",
    "    plot_radar_comparison(all_metrics, inference_speeds)\n",
    "\n",
    "def plot_radar_comparison(all_metrics, inference_speeds):\n",
    "    \"\"\"Crea un gr√°fico radar para comparaci√≥n multidimensional\"\"\"\n",
    "    \n",
    "    models = list(all_metrics.keys())\n",
    "    \n",
    "    # Normalizar m√©tricas para el radar\n",
    "    metrics_for_radar = ['mAP_50', 'mAP_50_95', 'precision', 'recall', 'f1_score']\n",
    "    labels = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Configurar √°ngulos\n",
    "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Cerrar el c√≠rculo\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        values = [all_metrics[model][metric] for metric in metrics_for_radar]\n",
    "        values += values[:1]  # Cerrar el c√≠rculo\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Configuraci√≥n del gr√°fico\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Comparaci√≥n Multidimensional de Modelos\\n(Gr√°fico Radar)', \n",
    "                 size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar visualizaciones\n",
    "plot_model_comparison(all_metrics, inference_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_per_class_metrics"
   },
   "outputs": [],
   "source": [
    "# Gr√°ficas de m√©tricas por clase\n",
    "def plot_per_class_metrics(all_class_metrics, class_names):\n",
    "    \"\"\"Crea gr√°ficas de m√©tricas por clase para cada modelo\"\"\"\n",
    "    \n",
    "    models = list(all_class_metrics.keys())\n",
    "    metrics = ['precision_per_class', 'recall_per_class', 'ap50_per_class']\n",
    "    metric_labels = ['Precision por Clase', 'Recall por Clase', 'AP@0.5 por Clase']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 15))\n",
    "    fig.suptitle('M√©tricas por Clase y Modelo', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for metric_idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "        ax = axes[metric_idx]\n",
    "        \n",
    "        for model_idx, model in enumerate(models):\n",
    "            if metric in all_class_metrics[model] and len(all_class_metrics[model][metric]) >= len(class_names):\n",
    "                values = all_class_metrics[model][metric][:len(class_names)]\n",
    "                bars = ax.bar(x + model_idx * width, values, width, \n",
    "                             label=model, color=colors[model_idx], alpha=0.8)\n",
    "                \n",
    "                # A√±adir valores en las barras\n",
    "                for bar, value in zip(bars, values):\n",
    "                    height = bar.get_height()\n",
    "                    if height > 0.01:  # Solo mostrar si el valor es significativo\n",
    "                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                               f'{value:.2f}', ha='center', va='bottom', \n",
    "                               fontsize=8, rotation=90)\n",
    "        \n",
    "        ax.set_xlabel('Clases')\n",
    "        ax.set_ylabel(label.split(' ')[0])\n",
    "        ax.set_title(label)\n",
    "        ax.set_xticks(x + width)\n",
    "        ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear gr√°ficas por clase\n",
    "plot_per_class_metrics(all_class_metrics, data_config['names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## 7. Inferencia en Im√°genes de Prueba\n",
    "\n",
    "Realizamos inferencia en 3-5 im√°genes de test con bounding boxes, clases y scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_demo"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para realizar inferencia y visualizar resultados\n",
    "def run_inference_comparison(models_dict, dataset_path, num_images=5):\n",
    "    \"\"\"Ejecuta inferencia con m√∫ltiples modelos y compara resultados\"\"\"\n",
    "    \n",
    "    test_images_dir = os.path.join(dataset_path, 'test', 'images')\n",
    "    test_images = os.listdir(test_images_dir)[:num_images]\n",
    "    \n",
    "    for img_idx, img_file in enumerate(test_images):\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        \n",
    "        print(f\"\nüîç INFERENCIA EN IMAGEN {img_idx + 1}: {img_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Cargar imagen original\n",
    "        original_img = cv2.imread(img_path)\n",
    "        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Crear subplot para comparaci√≥n\n",
    "        num_models = len(models_dict) + 1  # +1 for original\n",
    "        fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 6))\n",
    "        \n",
    "        # Mostrar imagen original\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title('Imagen Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Ejecutar inferencia con cada modelo\n",
    "        for model_idx, (model_name, model_path) in enumerate(models_dict.items(), 1):\n",
    "            # Cargar modelo\n",
    "            model = YOLO(model_path)\n",
    "            \n",
    "            # Realizar predicci√≥n\n",
    "            results = model(img_path, verbose=False)\n",
    "            \n",
    "            # Dibujar resultados\n",
    "            annotated_img = results[0].plot()\n",
    "            annotated_img = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[model_idx].imshow(annotated_img)\n",
    "            axes[model_idx].set_title(f'{model_name}')\n",
    "            axes[model_idx].axis('off')\n",
    "            \n",
    "            # Mostrar detalles de detecciones\n",
    "            print(f\"\n{model_name}:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            if len(results[0].boxes) > 0:\n",
    "                for box_idx, box in enumerate(results[0].boxes):\n",
    "                    # Extraer informaci√≥n\n",
    "                    conf = float(box.conf)\n",
    "                    cls = int(box.cls)\n",
    "                    class_name = data_config['names'][cls]\n",
    "                    bbox = box.xyxy[0].tolist()\n",
    "                    \n",
    "                    print(f\"  Detecci√≥n {box_idx + 1}:\")\n",
    "                    print(f\"    Clase: {class_name}\")\n",
    "                    print(f\"    Confianza: {conf:.3f}\")\n",
    "                    print(f\"    BBox: [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}]\")\n",
    "                    \n",
    "                    # [ESPACIO PARA IoU - Se calcular√° cuando ejecutes]\n",
    "                    print(f\"    IoU con GT: [COMPLETAR AL EJECUTAR]\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"  No se detectaron objetos\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # [ESPACIO PARA CAPTURAS - Guardar al ejecutar]\n",
    "        print(\"\nüì∏ [ESPACIO PARA CAPTURAS - Guardar imagen al ejecutar]\")\n",
    "        \n",
    "\n",
    "# Diccionario con modelos entrenados\n",
    "trained_models = {\n",
    "    'YOLOv8s': yolo_model_path,\n",
    "    'RT-DETR-L': rtdetr_model_path\n",
    "}\n",
    "\n",
    "# Ejecutar inferencia comparativa\n",
    "run_inference_comparison(trained_models, dataset_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iou_calculation"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular IoU con ground truth\n",
    "def calculate_iou_with_gt(pred_boxes, gt_boxes):\n",
    "    \"\"\"Calcula IoU entre predicciones y ground truth\"\"\"\n",
    "    \n",
    "    def box_iou(box1, box2):\n",
    "        # box format: [x1, y1, x2, y2]\n",
    "        # Calcular intersecci√≥n\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        \n",
    "        # Calcular √°rea de ambos boxes\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        \n",
    "        # Calcular uni√≥n\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    ious = []\n",
    "    for pred_box in pred_boxes:\n",
    "        max_iou = 0.0\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = box_iou(pred_box, gt_box)\n",
    "            max_iou = max(max_iou, iou)\n",
    "        ious.append(max_iou)\n",
    "    \n",
    "    return ious\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de c√°lculo de IoU definida\")\n",
    "print(\"\nüí° NOTA: Ejecuta las celdas anteriores para ver los resultados de inferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 8. Guardado de Resultados\n",
    "\n",
    "Guardamos todos los resultados en la carpeta /results para an√°lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_all_results"
   },
   "outputs": [],
   "source": [
    "# Crear carpeta de resultados\n",
    "results_dir = '/content/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Guardar m√©tricas en CSV\n",
    "results_table.to_csv(os.path.join(results_dir, 'metricas_comparativas.csv'), index=False)\n",
    "\n",
    "# Guardar m√©tricas detalladas en JSON\n",
    "import json\n",
    "\n",
    "detailed_results = {\n",
    "    'general_metrics': all_metrics,\n",
    "    'class_metrics': {model: {k: v.tolist() if hasattr(v, 'tolist') else v \n",
    "                              for k, v in metrics.items()} \n",
    "                      for model, metrics in all_class_metrics.items()},\n",
    "    'inference_speeds': inference_speeds,\n",
    "    'dataset_info': dataset_info\n",
    "}\n",
    "\n",
    "with open(os.path.join(results_dir, 'resultados_detallados.json'), 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "# Crear README de resultados\n",
    "readme_content = f\"\"\"\n",
    "# Resultados del Proyecto de Detecci√≥n PCB\n",
    "\n",
    "## Resumen de Archivos\n",
    "\n",
    "- `metricas_comparativas.csv`: Tabla comparativa de m√©tricas principales\n",
    "- `resultados_detallados.json`: M√©tricas completas y metadatos\n",
    "- `inferencias/`: Capturas de inferencias en im√°genes de test\n",
    "- `graficas/`: Visualizaciones de comparaci√≥n de modelos\n",
    "\n",
    "## Modelos Evaluados\n",
    "\n",
    "1. **YOLOv8s**: Modelo YOLO optimizado para velocidad\n",
    "2. **RT-DETR-L**: Detector basado en transformers en tiempo real\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- **Fuente**: Roboflow Universe - Printed Circuit Board\n",
    "- **Clases**: {len(data_config['names'])} ({', '.join(data_config['names'])})\n",
    "- **Total de im√°genes**: {dataset_info['train_size'] + dataset_info['val_size'] + dataset_info['test_size']}\n",
    "  - Entrenamiento: {dataset_info['train_size']}\n",
    "  - Validaci√≥n: {dataset_info['val_size']}\n",
    "  - Prueba: {dataset_info['test_size']}\n",
    "\n",
    "## Mejores Resultados\n",
    "\n",
    "[COMPLETAR AL EJECUTAR - Aqu√≠ se mostrar√°n los mejores resultados obtenidos]\n",
    "\n",
    "Generado el: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(results_dir, 'README.md'), 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "# Crear subdirectorio para inferencias\n",
    "os.makedirs(os.path.join(results_dir, 'inferencias'), exist_ok=True)\n",
    "os.makedirs(os.path.join(results_dir, 'graficas'), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Resultados guardados exitosamente en /content/results/\")\n",
    "print(f\"üìÅ Archivos generados:\")\n",
    "for file in os.listdir(results_dir):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Copiar a Google Drive si est√° montado\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    import shutil\n",
    "    gdrive_path = '/content/drive/MyDrive/PCB_Detection_Results'\n",
    "    shutil.copytree(results_dir, gdrive_path, dirs_exist_ok=True)\n",
    "    print(f\"\n‚òÅÔ∏è  Resultados tambi√©n guardados en Google Drive: {gdrive_path}\")\n",
    "except:\n",
    "    print(f\"\nüíæ Para guardar en Google Drive, monta tu drive primero\")\n",
    "\n",
    "print(f\"\nüéØ PROYECTO COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"üìä Revisa los archivos en /content/results/ para an√°lisis detallado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 9. Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Resumen de Resultados\n",
    "\n",
    "[COMPLETAR AL EJECUTAR - Aqu√≠ se incluir√°n las conclusiones basadas en los resultados obtenidos]\n",
    "\n",
    "### Modelo Ganador\n",
    "\n",
    "[COMPLETAR AL EJECUTAR - Modelo con mejor rendimiento general]\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "1. **Para aplicaciones en tiempo real**: Considerar el modelo con mejor FPS\n",
    "2. **Para m√°xima precisi√≥n**: Usar el modelo con mejor mAP\n",
    "3. **Para deployment en edge**: Evaluar el balance precisi√≥n-velocidad\n",
    "\n",
    "### Trabajo Futuro\n",
    "\n",
    "- Probar con m√°s √©pocas de entrenamiento\n",
    "- Experimentar con diferentes augmentaciones\n",
    "- Evaluar modelos de diferentes tama√±os (nano, medium, large)\n",
    "- Implementar ensemble de modelos\n",
    "- Optimizar para deployment (TensorRT, ONNX)\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Proyecto completado exitosamente!** üéâ\n",
    "\n",
    "Todos los resultados, gr√°ficas y an√°lisis est√°n guardados en `/content/results/` para revisi√≥n detallada."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
